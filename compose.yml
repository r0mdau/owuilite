services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:v0.6.33-slim
    ports:
      - "3000:8080"
    environment:
        GLOBAL_LOG_LEVEL: INFO
        OPENAI_API_BASE_URL: "http://litellm:4000"
        OPENAI_API_KEY: "sk-1234"
        RAG_OPENAI_API_BASE_URL: "http://litellm:4000"
        RAG_OPENAI_API_KEY: "sk-1234"
        RAG_EMBEDDING_ENGINE: "openai"
        RAG_EMBEDDING_MODEL: "text-embedding-3-small"
        IMAGES_OPENAI_API_BASE_URL: "http://litellm:4000"
        IMAGES_OPENAI_API_KEY: "sk-1234"
    volumes:
      - ./.data:/app/backend/data
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://openwebui:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - netopenwebui

  litellm:
    image: ghcr.io/berriai/litellm:main-v1.77.2-stable
    ports:
      - "4000:4000"
    environment:
        LITELLM_MASTER_KEY: "sk-1234"
        DATABASE_URL: "postgresql://litellm:password@db-litellm:5432/litellm"
        STORE_MODEL_IN_DB: "True"
    env_file:
      - ./litellm.env
    volumes:
      - ./litellm-config.yml:/app/proxy_server_config.yml
    command: ["--config", "/app/proxy_server_config.yml"]
    depends_on:
      db-litellm:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "wget","-q", "--no-verbose", "--tries=1", "http://litellm:4000/health/liveliness"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - netopenwebui

  db-litellm:
    image: postgres:16
    command: ["postgres", "-c", "log_min_messages=DEBUG1"]
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: password
    volumes:
      - db-litellm-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d litellm -U litellm"]
      interval: 1s
      timeout: 5s
      retries: 10
    networks:
      - netopenwebui

networks:
  netopenwebui:
    driver: bridge

volumes:
  db-litellm-data: